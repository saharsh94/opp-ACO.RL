README
OBL for Pheromone and Heuristic Updates (RL) in ACO

This repository contains the implementation of an innovative approach leveraging Opposition-Based Learning (OBL) to enhance the pheromone and heuristic updates Reinforcement Learning (RL) within an Ant Colony Optimization framework. The code, written in Python (ipynb), applies the algorithm to a provided dataset, which is included in the dataset directory. Please note that the associated paper detailing this methodology is currently under review and has not yet been published. Feedback and collaboration are welcome as the research progresses.
*****************************************************************************************

Abstract
This paper introduces an enhanced feature selection method for multi-label data using Ant Colony Optimization (ACO), augmented with Temporal Difference (TD) reinforcement learning and opposition-based learning (OBL). This novel integration of techniques aims to improve the efficiency and accuracy of feature selection. We propose three innovative algorithms that incorporate these methods, marking the first application of such a model to multi-label datasets. The proposed algorithms leverage ACO to navigate the feature space, while TD learning dynamically updates heuristic functions, specifically state value V, to better estimate future rewards for selected feature subsets. Additionally, OBL ensures comprehensive exploration by considering both original and opposite solutions. We evaluate our methods on nine diverse multi-label datasets using the Multi-Label K-Nearest Neighbors (MLKNN) classifier, with performance measured by accuracy and hamming loss. Experimental results demonstrate that our hybrid approach significantly outperforms traditional feature selection techniques, offering improved classification performance and more relevant feature subsets. This work establishes a new paradigm in multi-label feature selection, combining ACO, TD learning, and OBL for the first time.
